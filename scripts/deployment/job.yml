apiVersion: batch/v1
kind: Job
metadata:
  name: vsc
  namespace: 2446502gproject # Change this
spec:
  backoffLimit: 0
  template:
    metadata:
      name: vsc
    spec:
      containers:
      - name: vaejob-container
        image: notandres/pytorch-custom:1.7
        command: ["python3", "/nfs/scripts/trainer.py"]
        args: ["--base_dir", "$(BASE_DIR)", "--dataset_dir", "$(DATASET_DIR)", "--metadata_dir", "$(METADATA_DIR)",
               "--beta", "$(BETA)", "--epochs", "$(EPOCHS)", "--latent_dim", "$(LATENT_DIM)",
               "--num_workers", "$(NUM_WORKERS)", "--batch_size", "$(BATCH_SIZE)",
               "--alpha", "$(ALPHA)", "--c", "$(C)", "--model", "$(MODEL)"]
        resources:
          requests:
            cpu: "6000m"
            memory: "8Gi"
            nvidia.com/gpu: 1
          limits:
            cpu: "8000m"
            memory: "16Gi"
            nvidia.com/gpu: 1
        volumeMounts:
        - mountPath: /nfs
          name: nfs-access
        - mountPath: /dev/shm/
          name: dshm
        env:
        - name: "BASE_DIR"
          value: "/nfs/runs"
        - name: "DATASET_DIR"
          value: "/nfs/dataset/"
        - name: "METADATA_DIR"
          value: "/nfs/metadata/"
        - name: "MODEL"
          value: "vsc"
        - name: "BETA"
          value: "1"
        - name: "LATENT_DIM"
          value: "256"
        - name: "EPOCHS"
          value: "65"
        - name: "PYTHONUNBUFFERED"
          value: "1"
        - name: "PARALLEL"
          value: "False"
        - name: "NUM_WORKERS"
          value: "4"
        - name: "BATCH_SIZE"
          value: "128"
        - name: "RESUME"
          value: "True"
        - name: "ALPHA"
          value: "0.05"
        - name: "C"
          value: "250"
      volumes:
      - name: nfs-access
        persistentVolumeClaim:
          claimName: 2446502gvol1claim # Change this
      - name: dshm
        emptyDir:
          medium: Memory

      nodeSelector:
        node-role.ida/gputitan: "true"
      restartPolicy: Never
